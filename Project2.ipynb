{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56237cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 207260 entries, 0 to 207259\n",
      "Data columns (total 18 columns):\n",
      " #   Column                        Non-Null Count   Dtype \n",
      "---  ------                        --------------   ----- \n",
      " 0   Date received                 207260 non-null  object\n",
      " 1   Product                       207260 non-null  object\n",
      " 2   Sub-product                   164245 non-null  object\n",
      " 3   Issue                         207260 non-null  object\n",
      " 4   Sub-issue                     10347 non-null   object\n",
      " 5   Consumer complaint narrative  29391 non-null   object\n",
      " 6   Company public response       58458 non-null   object\n",
      " 7   Company                       207260 non-null  object\n",
      " 8   State                         205066 non-null  object\n",
      " 9   ZIP code                      197974 non-null  object\n",
      " 10  Tags                          28265 non-null   object\n",
      " 11  Consumer consent provided?    51313 non-null   object\n",
      " 12  Submitted via                 207260 non-null  object\n",
      " 13  Date sent to company          207260 non-null  object\n",
      " 14  Company response to consumer  207260 non-null  object\n",
      " 15  Timely response?              207260 non-null  object\n",
      " 16  Consumer disputed?            207260 non-null  object\n",
      " 17  Complaint ID                  207260 non-null  int64 \n",
      "dtypes: int64(1), object(17)\n",
      "memory usage: 28.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "df = pd.read_csv('complaints_25Nov21.csv')\n",
    "df.head()\n",
    "df.describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cdc3a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert y-variable to 0s and 1s \n",
    "## use label encoder \n",
    "le = preprocessing.LabelEncoder()\n",
    "X = df[['Product', 'Sub-product', 'Issue', 'State', 'Tags', 'Submitted via', 'Company response to consumer', 'Timely response?']].apply(le.fit_transform)\n",
    "y = le.fit_transform(df['Consumer disputed?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2592c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "if y_train.mean() < 0.3:\n",
    "    undersampler = RandomUnderSampler(random_state=123)\n",
    "    X_train, y_train = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11d8552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\studyresearch\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\studyresearch\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:13:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.52      0.64     32504\n",
      "           1       0.27      0.63      0.38      8948\n",
      "\n",
      "    accuracy                           0.55     41452\n",
      "   macro avg       0.55      0.58      0.51     41452\n",
      "weighted avg       0.71      0.55      0.59     41452\n",
      "\n",
      "[[17049 15455]\n",
      " [ 3315  5633]]\n"
     ]
    }
   ],
   "source": [
    "## Check what proportion of complaints in  training dataset are disputed.  \n",
    "## If this proportion is less than 30%, use random undersampling with random_state = 123 to balance dataset. \n",
    "from xgboost import XGBClassifier\n",
    "model_1 = XGBClassifier(random_state=123)\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = model_1.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "basecase_cost = y_test.sum() * 600 + (len(y_test) - y_test.sum()) * 100\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "modelcost = tn * 100 + fp * (100 + 90) + fn * 600 + tp * (100 + 90)\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "for threshold in thresholds:\n",
    "    y_predthreshold = (model_1.predict_proba(X_test)[:, 1] > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75288bf",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60d72db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of consumers raised a dispute is 0.21586413200810575\n"
     ]
    }
   ],
   "source": [
    "## We need to calculate the number of the disputes, and the total cases in the test set.\n",
    "## then we can calculate the proprotion which is dipute divided by cases.\n",
    "numdisputes = sum(y_test)\n",
    "totalcases = len(y_test)\n",
    "proportiondisputes = numdisputes / totalcases\n",
    "print('The proportion of consumers raised a dispute is', proportiondisputes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b27a46",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "795c0048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of consumers in the training dataset is 0.5\n"
     ]
    }
   ],
   "source": [
    "## We resamples the training data to balance the number of disputed and undisputed cases, making sure each category is equally represented. \n",
    "## Then calculateing and showing the percentage of disputed cases in this dataset.\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(random_state=123)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "num_disputes = sum(y_train_resampled)\n",
    "totalcases_resampled = len(y_train_resampled)\n",
    "proportiondisputesresampled = num_disputes / totalcases_resampled\n",
    "print('The proportion of consumers in the training dataset is' , proportiondisputesresampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ceac2d",
   "metadata": {},
   "source": [
    "## Question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7760bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\studyresearch\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\studyresearch\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:13:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "The recall for the category is 0.6295261510952168 it is between 0.6 and 0.67.\n"
     ]
    }
   ],
   "source": [
    "## We need to use under-sampling to address class imbalance. Then making predictions on the test set and calculating the cases marked as Disputed(YES), \n",
    "## Which measure the ability to identidy all actual disputed cases\n",
    "undersampler = RandomUnderSampler(random_state=123)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "model_2 = XGBClassifier(random_state=123)\n",
    "model_2.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model_2.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    " # Assuming '1' is'Yes'\n",
    "recalldisputed_yes = report['1']['recall'] \n",
    "print('The recall for the category is',recalldisputed_yes,'it is between 0.6 and 0.67.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebf801",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50540866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total cost to the banks of dealing with the complaints is 8619200\n"
     ]
    }
   ],
   "source": [
    "cost = len(y_test) * 100\n",
    "additionalcost = sum(y_test) * 500\n",
    "# Calculate the total cost\n",
    "totalcost = cost + additionalcost\n",
    "print('The total cost to the banks of dealing with the complaints is',totalcost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd05f1",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fec154f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the cost to the banks of dealing with the complaints in the test set is 7700620\n"
     ]
    }
   ],
   "source": [
    "cost_base = 100  \n",
    "cost_extra = 90  \n",
    "cost_dispute = 500  \n",
    "total_cost1 = ((tn + fp + fn + tp) * cost_base) + ((tp + fp) * cost_extra) + (fn * cost_dispute)\n",
    "total_cost1\n",
    "print('Now the cost to the banks of dealing with the complaints in the test set is',total_cost1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3aaa73",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "37628a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Total Cost based on the observations in the test set is: 6902720\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Predicted probabilities of the positive class\n",
    "predicted_probabilities = model_xgb.predict_proba(X_test)[:, 1]\n",
    "## We need to have the basic cost, dispute cost, investigation cost. Then initializing the variables to track  the best\n",
    "## threshold values from 0 to 1 to find the minimum total cost.\n",
    "\n",
    "basic_cost = 100 \n",
    "investigation_cost = 90 \n",
    "dispute_cost = 500  \n",
    "lowest_cost = np.inf  \n",
    "\n",
    "for threshold_value in np.linspace(0, 1, 101):\n",
    "    predictions_with_threshold = (predicted_probabilities >= threshold_value).astype(int)\n",
    "    true_neg, false_pos, false_neg, true_pos = confusion_matrix(y_test, predictions_with_threshold).ravel()\n",
    "    current_total_cost = (true_neg + false_pos + false_neg + true_pos) * basic_cost + \\\n",
    "                         (false_pos * investigation_cost) + (false_neg * dispute_cost)\n",
    "\n",
    "    if current_total_cost < lowest_cost:\n",
    "        lowest_cost = current_total_cost\n",
    "print(\"Lowest Total Cost based on the observations in the test set is:\", lowest_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8e69f",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b5ac3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest dollar cost achieved is: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Obtain probabilities for the positive class\n",
    "positive_class_probabilities = model_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "## We need to have the initial handling cost, additional investigation cost, and dispute cost.\n",
    "initial_handling_cost = 100  \n",
    "additional_investigation_cost = 90  \n",
    "dispute_resolution_cost = 500  \n",
    "\n",
    "## We need to start to make sure any first calculation is lower and also storing the decision yielding the lowest cost\n",
    "optimal_cost = np.inf \n",
    "optimal_decision_threshold = None \n",
    "\n",
    "## We want to find the most values by converting probabilities and also understanding the prediction outcomes, and compute\n",
    "## the total cost, investigationm and dispute costs.\n",
    "for decision_threshold in np.linspace(0, 1, 101):\n",
    "    predicted_outcomes = (positive_class_probabilities >= decision_threshold).astype(int)\n",
    "    true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_test, predicted_outcomes).ravel()\n",
    "    threshold_total_cost = (true_negative + false_positive) * initial_handling_cost + \\\n",
    "                           false_positive * additional_investigation_cost + \\\n",
    "                           false_negative * dispute_resolution_cost\n",
    "    if threshold_total_cost < optimal_cost:\n",
    "        optimal_cost = threshold_total_cost\n",
    "        optimal_decision_threshold = decision_threshold\n",
    "print(\"The lowest dollar cost achieved is:\", optimal_decision_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f5ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
